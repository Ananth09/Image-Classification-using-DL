{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtHuuMeuyaunmz9lK496OI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananth09/Pixel-Playground/blob/main/MobileNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_IHvuBimxDU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqGfoINSmyIU",
        "outputId": "16055af3-8aa9-401b-a547-588921177304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "qq0RrHdvmySe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAe517t5gS9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsUNrnxOg_G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIMUnM0DjHif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/img_class/'\n",
        "\n",
        "#parameters\n",
        "batch_size = 17\n",
        "image_size = (224, 224)\n",
        "num_epochs = 31"
      ],
      "metadata": {
        "id": "xYOETq_VmyVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range = 20,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    validation_split = 0.15\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "PO-i0Fc3jd6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkIonyjUjgX9",
        "outputId": "bed5c661-1f3a-40a0-da1a-e60026fa0272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1527 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znOLnkqmji3K",
        "outputId": "01ff43ba-3d1d-4b08-b299-37c12da62332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 268 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlYH5bS1myfV",
        "outputId": "e2321321-10f4-44f2-880a-606e50c0a345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "flatten_layer = Flatten()(base_model.layers[-1].output)\n",
        "output_layer=Dense(1, activation = 'sigmoid')(flatten_layer)\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)"
      ],
      "metadata": {
        "id": "fyFRUk4-myiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "rIjtHj2Gmykj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=val_generator,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeiL0_PqmynC",
        "outputId": "5114f9c8-4fff-4584-a3e4-bb20d5681a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/31\n",
            "90/90 [==============================] - 66s 339ms/step - loss: 0.1892 - accuracy: 0.9293 - val_loss: 1.2791 - val_accuracy: 0.6716\n",
            "Epoch 2/31\n",
            "90/90 [==============================] - 29s 323ms/step - loss: 0.0702 - accuracy: 0.9790 - val_loss: 1.0435 - val_accuracy: 0.7313\n",
            "Epoch 3/31\n",
            "90/90 [==============================] - 30s 328ms/step - loss: 0.0720 - accuracy: 0.9745 - val_loss: 1.6720 - val_accuracy: 0.6231\n",
            "Epoch 4/31\n",
            "90/90 [==============================] - 29s 319ms/step - loss: 0.0525 - accuracy: 0.9836 - val_loss: 0.7165 - val_accuracy: 0.8134\n",
            "Epoch 5/31\n",
            "90/90 [==============================] - 29s 323ms/step - loss: 0.0418 - accuracy: 0.9856 - val_loss: 0.6001 - val_accuracy: 0.8619\n",
            "Epoch 6/31\n",
            "90/90 [==============================] - 29s 322ms/step - loss: 0.0175 - accuracy: 0.9921 - val_loss: 0.2238 - val_accuracy: 0.8955\n",
            "Epoch 7/31\n",
            "90/90 [==============================] - 29s 322ms/step - loss: 0.0442 - accuracy: 0.9915 - val_loss: 0.4628 - val_accuracy: 0.8843\n",
            "Epoch 8/31\n",
            "90/90 [==============================] - 30s 328ms/step - loss: 0.0348 - accuracy: 0.9889 - val_loss: 0.8625 - val_accuracy: 0.8246\n",
            "Epoch 9/31\n",
            "90/90 [==============================] - 29s 324ms/step - loss: 0.0248 - accuracy: 0.9902 - val_loss: 1.0399 - val_accuracy: 0.8209\n",
            "Epoch 10/31\n",
            "90/90 [==============================] - 30s 330ms/step - loss: 0.0237 - accuracy: 0.9928 - val_loss: 1.1617 - val_accuracy: 0.8022\n",
            "Epoch 11/31\n",
            "90/90 [==============================] - 30s 337ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.4718 - val_accuracy: 0.8881\n",
            "Epoch 12/31\n",
            "90/90 [==============================] - 30s 335ms/step - loss: 0.0121 - accuracy: 0.9948 - val_loss: 0.2349 - val_accuracy: 0.9440\n",
            "Epoch 13/31\n",
            "90/90 [==============================] - 29s 325ms/step - loss: 0.0145 - accuracy: 0.9941 - val_loss: 0.3595 - val_accuracy: 0.9403\n",
            "Epoch 14/31\n",
            "90/90 [==============================] - 30s 328ms/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.3524 - val_accuracy: 0.9328\n",
            "Epoch 15/31\n",
            "90/90 [==============================] - 30s 334ms/step - loss: 0.0084 - accuracy: 0.9967 - val_loss: 0.2368 - val_accuracy: 0.9590\n",
            "Epoch 16/31\n",
            "90/90 [==============================] - 29s 328ms/step - loss: 0.0103 - accuracy: 0.9961 - val_loss: 0.1163 - val_accuracy: 0.9776\n",
            "Epoch 17/31\n",
            "90/90 [==============================] - 30s 330ms/step - loss: 0.0181 - accuracy: 0.9941 - val_loss: 0.2111 - val_accuracy: 0.9440\n",
            "Epoch 18/31\n",
            "90/90 [==============================] - 30s 337ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.2188 - val_accuracy: 0.9440\n",
            "Epoch 19/31\n",
            "90/90 [==============================] - 30s 328ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.1977 - val_accuracy: 0.9552\n",
            "Epoch 20/31\n",
            "90/90 [==============================] - 30s 335ms/step - loss: 0.0109 - accuracy: 0.9961 - val_loss: 0.1300 - val_accuracy: 0.9701\n",
            "Epoch 21/31\n",
            "90/90 [==============================] - 29s 328ms/step - loss: 0.0065 - accuracy: 0.9967 - val_loss: 0.2262 - val_accuracy: 0.9478\n",
            "Epoch 22/31\n",
            "90/90 [==============================] - 29s 324ms/step - loss: 0.0112 - accuracy: 0.9961 - val_loss: 0.6305 - val_accuracy: 0.8881\n",
            "Epoch 23/31\n",
            "90/90 [==============================] - 29s 323ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.5177 - val_accuracy: 0.9030\n",
            "Epoch 24/31\n",
            "90/90 [==============================] - 30s 328ms/step - loss: 0.0119 - accuracy: 0.9941 - val_loss: 0.2540 - val_accuracy: 0.9590\n",
            "Epoch 25/31\n",
            "90/90 [==============================] - 29s 328ms/step - loss: 0.0069 - accuracy: 0.9961 - val_loss: 0.1818 - val_accuracy: 0.9664\n",
            "Epoch 26/31\n",
            "90/90 [==============================] - 30s 329ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1702 - val_accuracy: 0.9739\n",
            "Epoch 27/31\n",
            "90/90 [==============================] - 30s 335ms/step - loss: 0.0148 - accuracy: 0.9961 - val_loss: 0.2466 - val_accuracy: 0.9664\n",
            "Epoch 28/31\n",
            "90/90 [==============================] - 29s 324ms/step - loss: 0.0270 - accuracy: 0.9921 - val_loss: 0.4802 - val_accuracy: 0.9478\n",
            "Epoch 29/31\n",
            "90/90 [==============================] - 29s 327ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.4513 - val_accuracy: 0.9552\n",
            "Epoch 30/31\n",
            "90/90 [==============================] - 29s 325ms/step - loss: 0.0209 - accuracy: 0.9941 - val_loss: 0.2299 - val_accuracy: 0.9664\n",
            "Epoch 31/31\n",
            "90/90 [==============================] - 29s 323ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 0.1463 - val_accuracy: 0.9664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mobilenetv2_2.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHVzoRSI7knS",
        "outputId": "6adb40c3-0ae9-4c06-e0c8-166a0406aca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation dataset\n",
        "val_loss, val_accuracy = model.evaluate(val_generator, verbose=1)\n",
        "\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PBJml7QR1lp",
        "outputId": "d46c42d3-7afa-4855-c976-0c4ba1c9c8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 4s 235ms/step - loss: 0.1866 - accuracy: 0.9478\n",
            "Validation Loss: 0.1866\n",
            "Validation Accuracy: 94.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model('mobilenetv2.h5')"
      ],
      "metadata": {
        "id": "fYxPAJ9dx_Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Define the folder path\n",
        "folder_path = '/content/drive/MyDrive/l(borderwhite)/'  # Replace with the actual path to your image folder\n",
        "\n",
        "# Initialize counters and a list to store paths\n",
        "lifestyle_count = 0\n",
        "plain_background_count = 0\n",
        "plain_background_paths = []\n",
        "lifestyle_img_paths=[]  # List to store paths of plain_background images\n",
        "predict_p=[]\n",
        "predict_l=[]\n",
        "# Loop through files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.jpg'):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        custom_image = load_img(image_path, target_size=(224, 224))\n",
        "        custom_image = img_to_array(custom_image)\n",
        "        custom_image = custom_image / 255.0\n",
        "        custom_image = custom_image.reshape(1, 224, 224, 3)\n",
        "\n",
        "        # Predict using your model\n",
        "        prediction = model.predict(custom_image)\n",
        "\n",
        "        # Interpret the prediction\n",
        "        if prediction > 0.5:\n",
        "            result = \"plain_background\"\n",
        "            plain_background_count += 1\n",
        "            plain_background_paths.append(image_path)  # Add the path to the list\n",
        "            predict_p.append(prediction)\n",
        "\n",
        "        else:\n",
        "            result = \"lifestyle\"\n",
        "            lifestyle_count += 1\n",
        "            lifestyle_img_paths.append(image_path)\n",
        "            predict_l.append(prediction)\n",
        "print(f\"Number of 'lifestyle' images: {lifestyle_count}\")\n",
        "print(f\"Number of 'plain_background' images: {plain_background_count}\")\n",
        "\n",
        "print(\"lifestyle treshold\")\n",
        "for tl in predict_l:\n",
        "  print(tl)\n",
        "\n",
        "print(\"\\n\\n\\nplain treshold\")\n",
        "for pl in predict_p:\n",
        "  print(pl)\n",
        "# # Print the paths of 'plain_background' images\n",
        "print(\"Paths of 'plain_background' images:\")\n",
        "for path in plain_background_paths:\n",
        "    print(path)\n",
        "\n",
        "print(\"\\nPaths of lifestyle images\")\n",
        "for p in lifestyle_img_paths:\n",
        "  print(p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVTk20UN0zsD",
        "outputId": "9ebe2949-07ea-4c6e-815b-a98f9a7589d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Number of 'lifestyle' images: 7\n",
            "Number of 'plain_background' images: 0\n",
            "lifestyle treshold\n",
            "[[5.047344e-15]]\n",
            "[[2.712993e-07]]\n",
            "[[4.3091674e-19]]\n",
            "[[3.4130202e-20]]\n",
            "[[4.5138968e-10]]\n",
            "[[0.03138745]]\n",
            "[[2.592843e-15]]\n",
            "\n",
            "\n",
            "\n",
            "plain treshold\n",
            "Paths of 'plain_background' images:\n",
            "\n",
            "Paths of lifestyle images\n",
            "/content/drive/MyDrive/l(borderwhite)/a3b1a2aLuminox-PH-W-0320_4.jpg\n",
            "/content/drive/MyDrive/l(borderwhite)/6a654b5th1791849_4.jpg\n",
            "/content/drive/MyDrive/l(borderwhite)/e33aebaP1006_4.jpg\n",
            "/content/drive/MyDrive/l(borderwhite)/912cb86BKPDSS304_4.jpg\n",
            "/content/drive/MyDrive/l(borderwhite)/MP000000000855154_437Wx649H_202212171734511.jpg\n",
            "/content/drive/MyDrive/l(borderwhite)/1c19c22SPRKL-BLK-BLK_5.jpg\n",
            "/content/drive/MyDrive/l(borderwhite)/0dc4925JWBS575_1.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "custom_image_path = '/content/drive/MyDrive/predicted_lifestyle/MP000000003336728_437Wx649H_20180629211849.jpeg'\n",
        "custom_image = load_img(custom_image_path, target_size=(224, 224))\n",
        "custom_image = img_to_array(custom_image)\n",
        "custom_image = custom_image / 255.0  # Normalize pixel values\n",
        "custom_image = custom_image.reshape(1, 224, 224, 3)  # Reshape for model input\n"
      ],
      "metadata": {
        "id": "HGdmDKX6mypc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "prediction = model.predict(custom_image)\n",
        "print(prediction)\n",
        "# Interpret the prediction\n",
        "if prediction > 0.5:\n",
        "    result = \"lifestyle\"\n",
        "else:\n",
        "    result = \"plain_background\"\n",
        "\n",
        "print(f\"The predicted class is: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmy6L3j-myr1",
        "outputId": "0ff0b939-3410-4c25-c158-909c80ee9b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 8s 8s/step\n",
            "[[9.203456e-07]]\n",
            "The predicted class is: plain_background\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTWQvMVImyuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFlxRkV-myxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qw7MRRbvmyzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCK8Y7r2my1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYPNz-02my4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yn1VYvrFmy7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}